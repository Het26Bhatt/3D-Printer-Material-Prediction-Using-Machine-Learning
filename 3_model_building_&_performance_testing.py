# -*- coding: utf-8 -*-
"""Milestone 3: Model Building & performance testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17-ZxvMw4_qM6owOWvoBOb6sc3S8PJs1M
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.naive_bayes import GaussianNB

data = pd.read_csv('/content/ADXL345_SensorData.csv')

data['Error_found']=data['Error_found'].map({'no': 0,'yes': 1})

X = data[['X-direction', 'Y-direction', 'Z-direction']]
y = data['Error_found']

# prompt: This code performs feature scaling on the input data using Min-Max scaling, which transforms the data to a specific range (usually between 0 and 1). This scaling is crucial because it ensures that all input features contribute equally to the model training process, preventing any particular feature from dominating due to its larger magnitude.

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Display the first few rows of the scaled data
print("Scaled data (first 5 rows):\n", X_scaled[:5])

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

y_train.value_counts()

from imblearn.over_sampling import SMOTE
smote = SMOTE()
X_train, y_train = smote.fit_resample(X_train, y_train)
y_train.value_counts()

log_reg = LogisticRegression()
log_reg.fit(X_train, y_train)

rf=RandomForestClassifier()
param_grid_rf={'n_estimators':[50,100,200],'max_depth':[None,10,20]}
grid_rf =  GridSearchCV(rf,param_grid_rf,cv=5)
grid_rf.fit(X_train,y_train)

svc = SVC()
param_grid_svc = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}
grid_svc = GridSearchCV(svc, param_grid_svc, cv=5)
grid_svc.fit(X_train, y_train)

# K-Nearest Neighbors (KNN)
knn = KNeighborsClassifier()
param_grid_knn = {'n_neighbors': [3, 5, 7], 'weights': ['uniform', 'distance']}
grid_knn = GridSearchCV(knn, param_grid_knn, cv = 5 )
grid_knn.fit(X_train, y_train)

xgb = XGBClassifier()
param_grid_xbg = {'max_depth':[3,5,7], 'learning_rate':[0.1,0.01,0.001]}
grid_xgb = GridSearchCV(xgb,param_grid_xbg,cv=5)
grid_xgb.fit(X_train,y_train)

nb = GaussianNB()
nb.fit(X_train, y_train)

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score


def evaluate_model(model, X_test, y_test, model_name):

 y_pred = model.predict(X_test)

 print(f"\n{model_name}:")
 print(classification_report(y_test, y_pred))

# Calculate and print accuracy
 accuracy = accuracy_score(y_test, y_pred)
 print(f"Accuracy: {accuracy:.4f}")

 try:
     roc_auc = roc_auc_score(y_test, y_pred)
     print (f"ROC AUC Score: {roc_auc:.4f}")
 except (ValueError, AttributeError):
     print("ROC AUC Score: Not applicable")



# Evaluate Logistic Regression
evaluate_model(log_reg, X_test, y_test, "Logistic Regression")

#Evaluate Random Forest (using best estimator from GridSearchCV)
evaluate_model(grid_rf.best_estimator_, X_test, y_test, "Random Forest")

#Evaluate Support Vector Classifier (using best estimator from GridSearchCV)
evaluate_model(grid_svc.best_estimator_, X_test, y_test, "Support Vector Classifier")

#Evaluate K-Nearest Neighbors (KNN) (using best estimator from GridSearchCV)
evaluate_model(grid_knn.best_estimator_, X_test, y_test, "K-Nearest Neighbors (KNN)")

# Evaluate XGBoost Classifier (using best estimator from GridSearchCV)t
evaluate_model (grid_xgb.best_estimator_, X_test, y_test, "XGBoost Classifier")

#Evaluate Naive Bayes
evaluate_model (nb, X_test, y_test, "Naive Bayes")

#Comparing Models
models = ['Logistic Regression', 'Random Forest', 'SVC', 'KNN', 'XGBoost', 'Naive Bayes']

def evaluate_model(model, X_test, y_test, model_name):
 y_pred = model.predict(X_test)

 print(f"\n{model_name}:")
 print(classification_report(y_test, y_pred))

 # Calculate and print accuracy
 accuracy = accuracy_score(y_test, y_pred)
 print(f"Accuracy: {accuracy:.4f}")

 try:
     roc_auc = roc_auc_score(y_test, y_pred)
     print (f"ROC AUC Score: {roc_auc:.4f}")
 except (ValueError, AttributeError):
     print("ROC AUC Score: Not applicable")

 return y_pred # Return the predictions


# Get predictions for each model
y_pred_log_reg = evaluate_model(log_reg, X_test, y_test, "Logistic Regression")
y_pred_rf = evaluate_model(grid_rf.best_estimator_, X_test, y_test, "Random Forest")
y_pred_svc = evaluate_model(grid_svc.best_estimator_, X_test, y_test, "Support Vector Classifier")
y_pred_knn = evaluate_model(grid_knn.best_estimator_, X_test, y_test, "K-Nearest Neighbors (KNN)")
y_pred_xgb = evaluate_model(grid_xgb.best_estimator_, X_test, y_test, "XGBoost Classifier")
y_pred_nb = evaluate_model(nb, X_test, y_test, "Naive Bayes")


accuracies = [
    accuracy_score(y_test, y_pred_log_reg),
    accuracy_score(y_test, y_pred_rf),
    accuracy_score(y_test, y_pred_svc),
    accuracy_score(y_test, y_pred_knn),
    accuracy_score(y_test, y_pred_xgb),
    accuracy_score(y_test, y_pred_nb)
]
plt.figure(figsize=(12, 6))
sns.barplot( x = models, y=accuracies)
plt.title('Model Comparison')
plt.ylabel('Accuracy')
plt.xticks(rotation=45)
plt.show()

best_model_index = np.argmax(accuracies)
best_model_name = models[best_model_index]
print(f"The best model is {best_model_name} with an accuracy of {accuracies[best_model_index]:.2f}.")

import numpy as np
from sklearn.metrics import accuracy_score

svc_model = grid_svc.best_estimator_

# Set seed for reproducibility
np.random.seed (42)

# Inform user about generating random test samples
print("Generating 10 random test samples with 3 features (X, Y, Z):")

#Create 10 test samples with 3 random values for X, Y, Z directions
X_test = np.random.rand(10, 3)

# Provide the correct answers for the test samples
# Replace this array with the actual correct labels for your test samples
y_test_actual = np.array([1, 0, 0, 1, 0, 1, 1, 1, 0, 0])

#Print the randomly generated samples for user reference
print("\nRandomly Generated Test Samples (X, Y, Z):")
for i in range(10):
  print(f"\tSample (i+1):", X_test[i])

# Predict the labels for the test samples using the trained model
y_test_pred = svc_model.predict(X_test)

# Print the model's predictions and the actual correct answers
print("\nPredictions vs Actual Labels:")
for i in range(len(y_test_actual)):
  print(f"Sample {i+1}: Prediction = {y_test_pred[i]}, Actual = {y_test_actual[i]}")

# Calculate and print the accuracy of the model
accuracy = accuracy_score(y_test_actual, y_test_pred)
print(f"\nModel Accuracy: {accuracy*100:.2f}%")

import pickle
pickle.dump(grid_svc,open('printergridsvc.pkl','wb'))
print('pickle model downloaded successfully')

